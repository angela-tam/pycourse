{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas\n",
    "import nilearn\n",
    "import numpy as np\n",
    "import scipy.stats as stat\n",
    "import nibabel as ni\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now lets use the tools we've learned to do a few demonstrations of actual operations you\n",
    "# might perform on a neuroimage. We will three examples: \n",
    "# 1) Extract values from an atlas\n",
    "# 2) Create a correlation matrix from a rsfMRI image, and extract networks from that image\n",
    "# 3) Voxelwise statistical operationn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## DEMONSTRATION 1: Extract values from an atlas ##\n",
    "# Lets say you have some PET images (or whatever) but you want to know the mean value within\n",
    "# the different ROIs of a given atlas. There are better atlases out there, but since everyone\n",
    "# uses it, lets use the Desikan-Killainy atlas (also know has the Freesurfer atlas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# First, lets load our image data and our atlas data\n",
    "\n",
    "cwd = os.getcwd()\n",
    "\n",
    "\n",
    "img = ni.load(\n",
    "        os.path.join(cwd,'stuff/nan_snorm_002-S-4229_18F-AV1451_2016-02-10_P4_I635352.nii.gz'))\n",
    "# Get individual subject PET data \n",
    "dat = img.get_data()\n",
    "\n",
    "# Get atlas\n",
    "jnk = ni.load(os.path.join(cwd,'stuff/dkt_atlas_1mm.nii.gz'))\n",
    "atlas = jnk.get_data()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now lets get our labels\n",
    "labels = pandas.read_csv(os.path.join(cwd,'stuff/dst_labels.csv'),header=None,\n",
    "                         skipinitialspace=True)\n",
    "labels.columns = ['label','roi']\n",
    "labels.index = labels['label'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# So, what we want to accomplish here is, for each roi, find all the values that are within\n",
    "# that ROI and average them. The ROIs are labeled by the atlas. Because the atlas and data are\n",
    "# in the same space, we can use indices from the atlas to index the PET data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Lets look at the unique values of the atlas -- in other words, lets look at all the\n",
    "# different labels.\n",
    "np.unique(atlas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Looks like there are a few extras (they are just part of the cerebellum so we dont care\n",
    "# about them). But also, notice the values are floats. Lets convert the atlas to integers so it\n",
    "# better matches our labels.\n",
    "atlas = atlas.astype(int)\n",
    "np.unique(atlas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Great. Now, there are a few ways to do this. One nice way would be to be smart with our masks\n",
    "# and indexing. This will use a For loop.\n",
    "def extract_values_with_atlas(dat,atlas,labels):\n",
    "    for lab in labels.index:\n",
    "        labels.ix[lab,'sub1_values'] = np.nanmean(dat[atlas==lab])\n",
    "\n",
    "# Let me explain the last line. \n",
    "\n",
    "# First, we are going to be updating the spreadsheet with the new values. By writing \n",
    "# labels.ix[lab,'sub1_values'] = x, we are saying change the value in the spreadsheet in \n",
    "# column 'sub1_values', at index lab, to x. The column sub1_values does not exist yet, so it \n",
    "# will be automatically created.\n",
    "\n",
    "# Next, when we say dat[atlas==lab], we are taking all values within certain coordinates in dat \n",
    "# -- specifically,  where the value is equal to lab at that coordinate in the atlas. In other\n",
    "# words, lets say for the first iteration, lab is 1, therefore referring to coordinates within\n",
    "# the caudal anterior cingulate. We are therefore taking only values in dat that are labeled\n",
    "# with a 1 in the atlas (the caudal anterior cingulate). This only works because the images are\n",
    "# in the same space and are the same size\n",
    "\n",
    "# Finally, once we have those values, we are averaging them. I chose to use np.nanmean, which\n",
    "# works quickly and ignores NaNs. If I didn't do this, the sum would be averaged by the total\n",
    "# number of voxels, including voxels with NaNs. This would be inaccurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# First we'll time it to get a sense of its speed\n",
    "%timeit extract_values_with_atlas(dat,atlas,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Mm.. not bad. On my machine it took 3.4s for all 80 ROIs -- ~25 ROIs a second. And how well\n",
    "# did it work?\n",
    "labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Quite well it seems! But can we make it faster? As we discussed before, the absolute most \n",
    "# efficient way to work on matrices in Python is to work with vectorized data. I.e. do\n",
    "# manipulations on an entire matrix at once, rather than iterating through each point on the\n",
    "# matrix. To do this, you need to either be good at math or be really clever. Here is an\n",
    "# an example -- I'll run it first and then explain how it works.\n",
    "\n",
    "def extract_values_vec(dat,atlas,labels,slc = None,col_lbl='vec_values'):\n",
    "    \n",
    "    count = np.bincount(atlas.flat)\n",
    "    tot = np.bincount(atlas.flat, weights=dat.flat)\n",
    "    if slc == None:\n",
    "        slc = [0,len(count)]\n",
    "    labels.ix[:,col_lbl] = (tot/count)[slc[0]:slc[1]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%timeit extract_values_vec(dat,atlas,labels,[1,81])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Much better! On my machine, I got about a 70x speed up! We did means on 80 ROIs in half a \n",
    "# second! And are the values the same?\n",
    "labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Okay so lets walk through exactly how this worked, because its a bit tricky. This method is\n",
    "# not very intuitive, but it makes use of existing python functions. Usually when you can do \n",
    "# that, you'll get better performance! Also notice there are no for loops and, in fact, there \n",
    "# is no explicit iteration whatsoever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lets look at the first line. What is this numpy function \"bincount\" doing?\n",
    "np.bincount?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Lets ignore the weight argument for now because its not used in the first line.\n",
    "\n",
    "# bincount basically assess the number of unique values in your matrix, and tells you how many\n",
    "# points in your matrix have that value. And it does so very quickly. \n",
    "\n",
    "# In our case, we apply it to our atlas, and the number of points in our matrix is actually the\n",
    "# number of voxels. So we are basically figuring out how many voxels equal each unique value.\n",
    "# Or in other words, the size (in voxels) of each label of the atlas!\n",
    "\n",
    "# Why are we doing this? Well, we know to calculate the mean, we divide the sum by the n. We \n",
    "# will be doing that separately for each region in the atlas, so this step is actually\n",
    "# collecting the \"n\" of each region! In other words, we're calculating the demoninator for each\n",
    "# mean calculation\n",
    "\n",
    "# Have a look\n",
    "count = np.bincount(atlas.flat)\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Interesting to note that ~80% of the atlas is made up of zeros (the first value in the array \n",
    "# above).\n",
    "\n",
    "# If this is difficult to understand, let's visualize it in a different way. Each value in the\n",
    "# atlas represents a region label. The array above, then, is the number of voxels in each \n",
    "# region\n",
    "\n",
    "# So:\n",
    "list(zip(labels.roi, count[1:80]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Next, we use np.bincount again, but this time, we make use of the \"weight\" argument. This \n",
    "# means that, instead of adding 1 for each cell for each unique value, we add 1*weight. Here,\n",
    "# we put dat (our image data) as the weight. That means that each time a cell is found to \n",
    "# \"belong\" to a unique value, instead of adding 1, we add 1 * the value of that same cell in \n",
    "# dat. So in other words, this is literally just a really clever (and fast!) way to sum all of \n",
    "# the values of dat, separately for each value in atlas. Its exactly what we want to do!\n",
    "\n",
    "# The result is a 83x1 array representing the sum of values in dat for each unique value in\n",
    "# atlas. So, we've already done away with the need to iterate.\n",
    "\n",
    "tot = np.bincount(atlas.flat, weights=dat.flat)\n",
    "tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now all we need to do is get the mean. Since we know the mean is sum / n, we just need to\n",
    "# divide our variable tot by the variable count, since count has the total number of cells for\n",
    "# each unique variable in the atlas. And since we don't want the first value (0) or the last\n",
    "# few values (81-82), we slice the final average so as to eliminate those values from the final\n",
    "# result\n",
    "avg = (tot/count)[1:81]\n",
    "print(avg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This would be a great time to let you guys do some experiments in working with imaging data.\n",
    "# You know how to utilize all of these data structures, you know many of the basic and built-in\n",
    "# Python commands, now its time to put them to the test.\n",
    "\n",
    "# Below we'll have some exercises where you will try to do just some very basic analyses of a\n",
    "# neuroimage. After that, we'll walk through some neuroimaging-specific Python functions, and\n",
    "# some more complicated analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# One more thing I'll teach you now is how to save an image. Its very easy with nibabel. All\n",
    "# you need is some image data and an affine that matches the data. Because we are not changing\n",
    "# the shape of the image, we can use the same affine of the image we loaded in the first place:\n",
    "aff = jnk.affine\n",
    "aff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now we save the image a a Nifti image:\n",
    "filename = os.path.join(cwd,'new_image')  # Define filename\n",
    "nimg = ni.Nifti1Image(dat,aff) # Create new image\n",
    "nimg.to_filename(filename) # Save new image to filename\n",
    "os.listdir() # List contents to see if the image was created\n",
    "\n",
    "# Feel free to open the image with your favorite image browser to see if worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now lets get rid of that image because we don't need it\n",
    "os.remove(filename+'.nii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This would be a great time to let you guys do some experiments in working with imaging data.\n",
    "# You know how to utilize all of these data structures, you know many of the basic and built-in\n",
    "# Python commands, now its time to put them to the test.\n",
    "\n",
    "# Below we'll have some exercises where you will try to do just some very basic analyses of a\n",
    "# neuroimage. After that, in Lesson 5C, we'll walk through some neuroimaging-specific Python \n",
    "# functions, and some more complicated analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###### EXERCISES ##########\n",
    "\n",
    "# NOTE: This time around, I've added some prompts to help you figure out what steps need to be\n",
    "# taken\n",
    "\n",
    "## PART A\n",
    "# Write an image thresholding Python function. This will take a path to an image, a \n",
    "# threshold value, and an output name, as inputs. The function will threshold the image by \n",
    "# setting all voxels that are less than the supplied threshold to 0. Finally, the function will \n",
    "# write a new image which has been \"thresholded\". Test your function on the existing PET data \n",
    "# and look at the image you created to see if it works!\n",
    "\n",
    "## PART B\n",
    "# Threshold the imaging data such that only the top 5% of voxel values remain. Save this image\n",
    "# and view it to see where the highest PET values are in the brain.\n",
    "\n",
    "# Find top 5% index\n",
    "\n",
    "# Find top 5% value\n",
    "\n",
    "# Threshold image\n",
    "\n",
    "# Write image to file\n",
    "\n",
    "## PART C\n",
    "# You have a hypothesis that PET signal will be higher in the Putamen than in the cortex.\n",
    "# Create two matrices -- one containing flattened voxel values from the putamen, and another\n",
    "# containing flattened voxel values from the rest of the brain. Make sure you don't get\n",
    "# voxels with labels of 0, because those are outside the brain! Print the means of the two\n",
    "# matrices. Then, run a t-test between these vectors to test your hypothesis. \n",
    "# NOTE: There are two Putamen ROIs, left and right. You'll want to combine values from both\n",
    "\n",
    "# Create putamen matrix\n",
    "\n",
    "# Create matrix for the rest of the brain\n",
    "\n",
    "# Print means\n",
    "\n",
    "# Run t-test\n",
    "\n",
    "\n",
    "## PART D\n",
    "# Using your favorite automation technique, run a t-test between the left Inferior Temporal \n",
    "# and every brain region. Then, create an image where regions are set to 0 if the average PET \n",
    "# signal in that region is significantly lower than the average Putamen signal, and otherwise \n",
    "# set the region to 1. \n",
    "# NOTE: For the labels, all of the left ROIs appear first, and right ROIs appear\n",
    "# second.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### ANSWERS BELOW ####\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### ANSWERS TO EXERCISES\n",
    "\n",
    "## PART A\n",
    "# Write an image thresholding Python function. This will take a path to an image, a \n",
    "# threshold value, and an output name, as inputs. \n",
    "\n",
    "def threshold_image(image, thresh, outname):\n",
    "    '''This function will threshold an image by a given value and return the thresholded\n",
    "    image. Image should be a path to an existing image. thresh should be a number. outname\n",
    "    should be string representing the filename of the new image to be created (without\n",
    "    extension)'''\n",
    "\n",
    "    # The function will threshold the image by setting all voxels that are less than the \n",
    "    # supplied threshold to 0. \n",
    "    jnk = ni.load(image)\n",
    "    i_data = jnk.get_data()\n",
    "    aff = jnk.affine\n",
    "    \n",
    "    i_data[i_data<thresh] = 0\n",
    "    \n",
    "    # Finally, the function will write a new image which has been thresholded. \n",
    "    nimg = ni.Nifti1Image(i_data,aff)\n",
    "    nimg.to_filename(outname)\n",
    "    \n",
    "# Test your function on the existing PET data and look at the image you created to see if it \n",
    "# works!\n",
    "file_in = os.path.join(cwd,\n",
    "                       'stuff/nan_snorm_002-S-4229_18F-AV1451_2016-02-10_P4_I635352.nii.gz')\n",
    "threshold_image(file_in, 0.8, 'thresh_image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## PART B\n",
    "# Threshold the imaging data such that only the top 5% of voxel values remain. Save this image\n",
    "# and view it to see where the highest PET values are in the brain.\n",
    "\n",
    "# Find top 5% index:\n",
    "flat_dat = dat.flatten()\n",
    "img_size = len(flat_dat)\n",
    "ind_5p = round(img_size * 0.95)\n",
    "\n",
    "# Find top 5% value:\n",
    "flat_dat.sort()\n",
    "thr_val = flat_dat[ind_5p]\n",
    "thr_val\n",
    "\n",
    "# Threshold\n",
    "tdat = deepcopy(dat) \n",
    "tdat[tdat<thr_val] = 0\n",
    "\n",
    "# Write new image\n",
    "nimg = ni.Nifti1Image(tdat,aff)\n",
    "nimg.to_filename(os.path.join(cwd,'ex2_image'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## PART C\n",
    "# You have a hypothesis that PET signal will be higher in the Putamen than in the cortex.\n",
    "\n",
    "\n",
    "# Create two matrices -- one containing flattened voxel values from the putamen,\n",
    "put_idx = [i for i in labels.index if labels.ix[i,'roi'] == 'Putamen']\n",
    "put_mat = dat[(atlas==put_idx[0]) | (atlas==put_idx[1])]\n",
    "\n",
    "# and another containing flattened voxel values from your the rest of the brain. \n",
    "n_atlas = deepcopy(atlas)\n",
    "n_atlas[n_atlas == put_idx[0]] = 0\n",
    "n_atlas[n_atlas == put_idx[1]] = 0\n",
    "ctx_mat = dat[n_atlas > 0]\n",
    "\n",
    "# Print the means of the two matrices\n",
    "print(np.mean(put_mat))\n",
    "print(np.mean(ctx_mat))\n",
    "\n",
    "# Run a t-test between these vectors to test your hypothesis.\n",
    "stat.ttest_ind(put_mat,ctx_mat)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## PART D\n",
    "# Using your favorite automation technique, run a t-test between the Inferior Temporal \n",
    "# and every brain region. Then, create an image where regions are set to 0 if the average PET \n",
    "# signal in that region is significantly lower than the average Putamen signal, and otherwise \n",
    "# set the region to 1. \n",
    "\n",
    "# There are many, many ways to do this. This is just one approach.\n",
    "\n",
    "print('initializing')\n",
    "it_idx = [i for i in labels.index if labels.ix[i,'roi'] == 'Inferior temporal'][0]\n",
    "it_mat = dat[atlas==it_idx]\n",
    "\n",
    "# Find which ROIs are significantly different\n",
    "print('finding significant ROIs')\n",
    "sig_indx = [i for i in labels.index if stat.ttest_ind(\n",
    "                                                dat[atlas==i],\n",
    "                                                it_mat)[1] < (0.05/80)]\n",
    "\n",
    "\n",
    "# Of those, see which ROIs have lower average SUVRs using the spreadsheet we created earlier\n",
    "print('refining...')\n",
    "sig_indx = [i for i in sig_indx if labels.ix[i,\n",
    "                                               'sub1_values'] < labels.ix[it_idx,\n",
    "                                                                          'sub1_values']]\n",
    "# Create the new image by binarizing the regions\n",
    "print('binarizing')\n",
    "thr_dat = deepcopy(dat)\n",
    "n_atlas = deepcopy(atlas)\n",
    "\n",
    "\n",
    "# And binarize...\n",
    "for ind in sig_indx:\n",
    "    n_atlas[n_atlas == ind] = 0\n",
    "thr_dat[n_atlas == 0] = 0\n",
    "thr_dat[n_atlas > 0] = 1\n",
    "\n",
    "# Write image to file\n",
    "nimg = ni.Nifti1Image(thr_dat,aff)\n",
    "nimg.to_filename('ex4_image')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Clean up\n",
    "\n",
    "i_2_del = ['ex2_image.nii','ex4_image.nii','thresh_image.nii']\n",
    "for i in i_2_del:\n",
    "    try:\n",
    "        os.remove(i)\n",
    "    except:\n",
    "        continue\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py3]",
   "language": "python",
   "name": "conda-env-py3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
