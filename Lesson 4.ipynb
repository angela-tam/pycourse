{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######## LESSON 4: BASIC DATA ANALYSIS #########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''In this lesson, we will go over how to use python to perform basic statistical\n",
    "analysis and how to visualize your results. We will also go over ways to use \n",
    "Python to do simple analyses that GUI-based programs may not be able to do'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### PREAMBLE: WHY PYTHON FOR DATA ANALYSIS? ###\n",
    "\n",
    "# You may be thinking, why would I use Python for basic data analysis when I can do it \n",
    "# quickly and expertly in, say, SPSS. Sometimes, many users will find that Python is better\n",
    "# suited for certain moments in an analysis, while a GUI-based program might be better for\n",
    "# others. You might even only use Python to prepare your data, and perform all the analysis with\n",
    "# some other software. That's all fine. But I'd like to list a few situations/reasons why \n",
    "# Python might be a good alternative, even in the simplest cases, despite a small drop-off\n",
    "# in speed in some cases.\n",
    "\n",
    "# 1) CONTROL: Ultimately, your GUI-based software will have some limitations, whether its with\n",
    "# data analysis, preparation, of visualization. In Python, you literally have full control \n",
    "# over everything. That doesn't mean it will be easy to, for example, get exactly the figure\n",
    "# you want, but the point is that its possible. You have full control over the data and figures\n",
    "# and don't need to worry about whether or not there's a button or function that does what you\n",
    "# want -- you can always do it by hand if you have to.\n",
    "\n",
    "# 2) DOCUMENTATION: Performing your analysis in a Jupyter notebook has an enormous advantage of\n",
    "# documenting everything you did and how you do it. This can make replicating previous analyses\n",
    "# easy, and can help in sharing, displaying and saving everything you've done in an analsis.\n",
    "# As you'll see below, Jupyter has excellent support for visualizations and plots\n",
    "\n",
    "# 3) AUTOMATION: You have all of the Python automation tools at your disposal. That means you can\n",
    "# quickly perform the same action numerous times and create clever algorithms to transform or\n",
    "# analyze your data however you want. This can save you a lot of time in data wrangling and \n",
    "# preparation, and opens the door to new types of analyses. Also, the ability to define new\n",
    "# functions can make short work of processes that may be cumbersome using GUI-based software\n",
    "\n",
    "# 4) FLEXIBILITY: Somewhere out there, there's a python function for the analysis you want to \n",
    "# do. Again, your GUI-based software has a limit to its analysis capacity, whereas Python has \n",
    "# an Internet of contributors. If your software can't do it, Python almost surely can. And if \n",
    "# not, you have the option to build your function from scratch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In order to complete this lesson, you'll need to download and install a few libraries\n",
    "# that don't come standard with Anaconda package. Fortunately, its very easy to download\n",
    "# packages using Anaconda.\n",
    "\n",
    "# The packages you'll need are the following:\n",
    "\n",
    "# seaborn    # data visualization\n",
    "# matplotlib # data visualization\n",
    "\n",
    "# A handy resource for installing packages with conda can be found here: \n",
    "# https://conda.io/docs/using/pkgs.html\n",
    "# In addition, conda's errors are very user-friendly and straightforward\n",
    "# so you could probably figure out how to do this for yourself. But, in \n",
    "# the interest of time, simply do the following\n",
    "\n",
    "# 1) Open a terminal or command-prompt \n",
    "# 2) Type conda install [package]\n",
    "\n",
    "# That's it! If it doesn't work, conda will tell you why and will provide\n",
    "# helpful clues for making it happen.\n",
    "\n",
    "# You should also update the package statsmodels:\n",
    "# conda update statsmodels\n",
    "\n",
    "# You may need to restart your kernel for the changes to apply\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# First, we'll import some modules that we will need for this lesson\n",
    "import os,sys\n",
    "import pandas\n",
    "import statsmodels\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "# notice I'm importing this next module *as* stat. this is a shortcut so I don't have to \n",
    "# type scipy.stats each time\n",
    "import scipy.stats as stat \n",
    "\n",
    "# again I'm loading these libraries into simpler names\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We will use two visualization libraries: matplotlib and seaborn. matplotlib is a bit finicky\n",
    "# and not entirely intuitive, but its extremely dynamic and allows for very tight control over\n",
    "# plots. In contrast, seaborn is actually built on top of matplotlib. It is very easy to use \n",
    "# and creates very nice looking figures with good documentation, but I sometimes find it less \n",
    "# suitable for making very specific plots. This comparison is very similar to the contrast \n",
    "# between numpy and pandas (which is built on top of numpy)\n",
    "\n",
    "# Lets also customize our plots so they default to a special style that's been designed to\n",
    "# look appealing (the default style is very bland)\n",
    "\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# In addition, we'll need to import the scripts we created at the end of Lesson 3B.\n",
    "# I put all of these scripts into a new text document that I called\n",
    "# pycourse_lesson3_4_scripts.py. Because it is formatted in a python-friendly way,\n",
    "# and because it has a .py extension, python should be able to read it like it was\n",
    "# any other python library.\n",
    "\n",
    "# Feel free to have a look at our new \"library\" by opening it with any text editor\n",
    "# (I always recommend Sublime Text, its worth downloading if you don't have it).\n",
    "\n",
    "# Because the file containing our scripts is in our current working directory, we \n",
    "# can import it like any other library.\n",
    "\n",
    "import pycourse_lesson3_4_scripts as us # us because WE did it!\n",
    "\n",
    "# NOTE: if the script was somewhere else on your hard drive, you would have to \n",
    "# change your PYTHONPATH first before you imported the script. Luckily, that's\n",
    "# easy, and would look something like this:\n",
    "\n",
    "# sys.path.insert(0,'/path/to/directory/containing/function')\n",
    "# import function\n",
    "\n",
    "# NOTE: Feel free to update this library with your own functions if you think it works better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now that we've imported our functions, they are in our namespace, and we can access\n",
    "# them in the same way we access other functions from other libraries. Have a look:\n",
    "\n",
    "# You can browse the functions here by putting your cursor next to the . and pressing tab\n",
    "\n",
    "# us.\n",
    "\n",
    "# The commands also have docstring\n",
    "us.binarize?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Okay, lets get started. There are many, many, many available libraries and functions\n",
    "# within those libraries to perform basic (and very complicated) stats. Luckily, most\n",
    "# of them are pretty intuitive, such that, if you know the stats, you should be able\n",
    "# to use the functions without much diffuculy.\n",
    "\n",
    "# In the scipy.stats module alone, there are implementations of many of the statistical\n",
    "# tests and transformations one would need for basic statistical analysis. Have a look:\n",
    "\n",
    "# stat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Because of that, it would be silly to go over each one with you, as it won't be hard\n",
    "# to figure out on your own. Instead, I'll go through a series of \"Demonstrations\" that will\n",
    "# showcase some of the features of these various stats and visualization libraries\n",
    "\n",
    "# DEMO 1: Basic correlations and plotting functions for linear relationships and histograms\n",
    "# DEMO 2: T-tests, cross-tabs + chi-square, and visualizations, permutation test, FDR correction\n",
    "# DEMO 3: Multivariate linear models and multivariate visualization\n",
    "\n",
    "# First, lets load our dataframe...\n",
    "\n",
    "sheet = os.path.join(os.getcwd(),'stuff/pycourse_survey.csv')\n",
    "df = pandas.read_csv(sheet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# DEMONSTRATION 1: How random is \"random\"\n",
    "\n",
    "''' Here, I'll select a few random (literally) variables to show off some basic visualization\n",
    "tools for distributions and simple linear models. The results aren't too interesting but the\n",
    "graphs are pretty! '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A WORD ON PLOTTING\n",
    "\n",
    "# The way plots work in python is like this: First you generate the plot, then you show it.\n",
    "# The plot will not be visualized until you type matplotlib.pyplot.show() -- or in our case,\n",
    "# plt.show(). This may seem annoying, but it allows a fair degree of control. For example, you\n",
    "# can create multiple overlaying plots, and then, when you're ready, you can type plt.show()\n",
    "# to visualize the whole thing.\n",
    "\n",
    "# You may end up accidentally creating several plots before showing them, and when you do\n",
    "# plt.show(), you will actually visualize multiple graphs, often the same graph. This is\n",
    "# often because you initialized several plots by accident. You can simply run plt.close() to\n",
    "# close plots. In the notebook below, I will often type plt.close() before I start a plot just \n",
    "# in case I (or you) accidentally opened some extra plots, and to prevent plotting over \n",
    "# existing figures instead of forming new figures\n",
    "\n",
    "# If you're working on a non-notebook interactive python environment like Spyder of iPython,\n",
    "# you may want to have plots generate automatically without having to type plt.show(). To do\n",
    "# that, simply type the following (including the %)\n",
    "\n",
    "# %matplotlib inline \n",
    "\n",
    "# But just a warning, it can sometimes be a bit hard to control in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# First, lets have a look at the distribution and get some statistics about it..\n",
    "# If its truly random, we would expect the distribution to be fairly random (though perhaps\n",
    "# not since our sample size may be too low for this to manifest given our dynamic range...)\n",
    "\n",
    "# set column to a variable\n",
    "rand1 = df['Pick a number from 1 to 100']\n",
    "\n",
    "# plot the histogram\n",
    "fig = rand1.plot.hist() # declare figure, in this case, a histogram\n",
    "plt.show() # show it\n",
    "\n",
    "# let's also get some distribution statistics\n",
    "print('the skew is %s and the kurtosis is %s'%(rand1.skew(),rand1.kurtosis()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We can test if the skew and kurtosis are different from that of a normal distribution\n",
    "\n",
    "# Test the skewness:\n",
    "print(stat.skewtest(rand1))\n",
    "\n",
    "# Test the kurtosis\n",
    "print(stat.kurtosistest(rand1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Alright, so the skew is normal but the kurtosis is highly abnormal. This is easy to see\n",
    "# if we print the shape of the historgram\n",
    "rand1.plot.kde()\n",
    "plt.xlim(-20,120) # lets set the axes to where we want them...\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# alternatively, we can view them both on top of one another using seaborn\n",
    "#plt.close()\n",
    "sns.distplot(rand1.tolist(),bins=10,axlabel='Randomly chosen numbers')\n",
    "plt.xlim(-20,120)\n",
    "plt.show()\n",
    "# Note that the x-xis changed is changed to a density measure rather than an absolute count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Its clear to see the distribution curve is very wide and without an obvious peak\n",
    "\n",
    "# We asked the survey-takers to input a random number later on in the survey. Lets have\n",
    "# a look at how this variable compares to our first random number choice...\n",
    "\n",
    "rand2 = df['Pick another number from 1 to 100']\n",
    "print(rand2.head())\n",
    "\n",
    "# However, when we look at the dtype of the object, we see it is an \"object\". In contrast,\n",
    "# the dtype for rand1 is a float. Dtype is a special categories of type thats specific to \n",
    "# numpy variables. Just think of it like another version of type for now (they often overlap)\n",
    "print(rand1.head())\n",
    "print('\\nare rand1 and rand2 the same dtype? %s'%(rand1.dtype == rand2.dtype))\n",
    "\n",
    "# If the column is an \"object\" instead of a float, it may mean that there is some sort of\n",
    "# non-numeric value within the column that caused pandas to not recognize rand2 should be\n",
    "# a column of floats.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Okay let's try to set the dtype of rand2 to float\n",
    "rand2 = rand2.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# That's a looooong error but the very end says what we need to know. There is a value called\n",
    "# \"Pi\" within our column.. some joker typed Pi instead of a number! Just as we suspected..\n",
    "\n",
    "# Lets use one of the functions we designed last lesson to have a look at where this occured.\n",
    "bad_idx = us.purify_numbers(rand2,mode='evaluate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Normally, we could use this same function to turn this into a NaN, but actually, pi *is* \n",
    "# a real number. So lets assume that's what the user wanted and replace that value.\n",
    "\n",
    "from math import pi\n",
    "rand2[bad_idx] = pi\n",
    "\n",
    "print(rand2[bad_idx])\n",
    "\n",
    "# Looks good, now lets try again to convert the column to a column of floats...\n",
    "try:\n",
    "    rand2 = rand2.astype(float)\n",
    "    print('\\nthe conversion worked! \\n')\n",
    "except:\n",
    "    print('\\nnope didn\\'nt work \\n')\n",
    "          \n",
    "print('dtype of rand2 is now %s, is it the same as rand1? %s'%(\n",
    "                rand2.dtype,rand2.dtype == rand1.dtype))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Alright, now that we have that out of the way, lets have a look at an overlay \n",
    "# of the histogram for these two \"random\" variables using seaborn. In this case, we need only\n",
    "# to declare both plots before visualizing.\n",
    "\n",
    "# Plot 2 histograms, remove NaNs where appropriate, add a legend \n",
    "plt.close() # Close existing plots\n",
    "sns.distplot(rand1.dropna(), bins=10, label='Rand1') # Hist 1\n",
    "sns.distplot(rand2.dropna(), bins=10, label='Rand2') # Hist 2\n",
    "plt.xlim(-20,120)\n",
    "plt.legend() # Add a legend\n",
    "plt.show() # Let's see it\n",
    "\n",
    "# And lets test the normality of rand2 as well\n",
    "print(stat.skewtest(rand2))\n",
    "print(stat.kurtosistest(rand2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Perhaps things will be a bit easier to visualize if we look at only the curves\n",
    "plt.close()\n",
    "sns.kdeplot(rand1.dropna(), label='Rand1')\n",
    "sns.kdeplot(rand2.dropna(), label='Rand2')\n",
    "plt.xlim(-20,120)\n",
    "plt.legend()\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Looks the same: normal skewness, non-normal kurtosis.\n",
    "# Are these two \"random\" variables related to one another?\n",
    "r,p = stat.pearsonr(rand1,rand2)\n",
    "print('r is %s, p is %s'%(r,p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Lets have a look using a nifty seaborn plot that shows both points and the distribution of\n",
    "# each variable.  \n",
    "\n",
    "plt.close() # close existing plot\n",
    "sns.jointplot(rand1,rand2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A relationships is indeed hard to see. Lets use a different type of seaborn plot\n",
    "# to see what direction the relationship is going (even though the relationships is not\n",
    "# significant)\n",
    "\n",
    "# Unlike the previous plot, we cannot just put in the variables we want. Instead, seaborn\n",
    "# expects a dataframe and column names (this is very similar to R). So, lets construct a\n",
    "# mini Dataframe, make the columns easier to deal with, and then make the plot.\n",
    "\n",
    "plt.close() \n",
    "# make a mini dataframe\n",
    "forplot = pandas.concat([rand1,rand2], # concatenate these two pandas Series'\n",
    "                        axis=1, # concatenate so the columns are next to one another, not stacked\n",
    "                        join_axes=[rand1.index]) # make the new index the index of rand 1\n",
    "forplot.columns = ['rand1','rand2'] # change columns\n",
    "\n",
    "sns.lmplot('rand1','rand2',data=forplot) # plot. Notice I'm now plotting by column names and passing\n",
    "                                         # the dataframe to specify where the colums point to\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# But wait! Didn't we discuss the fact that these variables are not normal! That means we can't\n",
    "# use traditional parametric statistics. Lets use spearman's rank correlations instead...\n",
    "\n",
    "rho, p = stat.spearmanr(rand1,rand2)\n",
    "print('rho is %s, p is %s'%(rho,p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Still not significant, though its a bit closer. Lets have a look at the data, though\n",
    "# recall we'll have to rank the data first, since its a spearman's correlation\n",
    "ranked_rand1 = pandas.Series(stat.rankdata(rand1,method='min')) # convert to Series...\n",
    "ranked_rand2 = pandas.Series(stat.rankdata(rand2,method='min'))\n",
    "\n",
    "print(ranked_rand1.head())\n",
    "print(ranked_rand2.head())\n",
    "\n",
    "# see the docstring to find out why I chose method = min\n",
    "stat.rankdata?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# And now we prepare the plot...\n",
    "plt.close()\n",
    "forplot_r = pandas.concat([ranked_rand1,ranked_rand2], axis=1, join_axes=[rand1.index]) # make df\n",
    "forplot_r.columns = ['ranked_rand1','ranked_rand2'] # change columns\n",
    "sns.lmplot('ranked_rand1','ranked_rand2',forplot_r) # plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# It would be nice to overlay them.. Lets have a look!\n",
    "\n",
    "print('rand1 is blue, rand2 is red')\n",
    "plt.close()\n",
    "fig, ax = plt.subplots() # Telling plt that I want two different plots on the same figure\n",
    "sns.regplot('rand1','rand2',forplot,ax=ax,color='b',label='rand1') # Plot 1\n",
    "ax2 = ax.twinx() # Add a second axis\n",
    "sns.regplot('ranked_rand1','ranked_rand2',forplot_r,ax=ax2,color='r',label='rand2') # Plot2\n",
    "# notice plot 2 is set to the second axis. This is because these two variables cannot share\n",
    "# an axis -- they are on different scales\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# I suppose thats cool, but the labeling is all weird. \n",
    "# We *could* mess around with attributes of ax1 and ax2 and use other matplotlib functions \n",
    "# to fix this -- you can do almost anything with this library, though it takes some \n",
    "# getting used to.\n",
    "\n",
    "# Instead, lets exploit the type of data structures seaborn expects to get the plot we want.\n",
    "# We will stack the two mini-dataframes on top of one another and create a new column \n",
    "# describing which points are \"ranked values\" vs. which are not.\n",
    "\n",
    "### PREPARE DF\n",
    "# In order to merge these DFs, we need make sure their columns are the same\n",
    "forplot_r.columns = forplot.columns\n",
    "\n",
    "# and we need to change the indices to be unique\n",
    "df_len = len(forplot.index)\n",
    "forplot_r.index = range(df_len,df_len*2)\n",
    "\n",
    "\n",
    "## MERGE DFs\n",
    "\n",
    "for_2plot = pandas.concat([forplot,forplot_r]) # now we merge them\n",
    "print(for_2plot.head(),'with shape', for_2plot.shape,'\\n')\n",
    "\n",
    "\n",
    "## MAKE NEW VARIABLE\n",
    "for i in for_2plot.index: # iterate through the indices.\n",
    "    if i < len(forplot): # if the index is in the first n, where n=length of first first df\n",
    "        for_2plot.ix[i,'ranked?'] = 'Parametric' # set the value in the column \"ranked?\"\n",
    "    else:\n",
    "        for_2plot.ix[i,'ranked?'] = 'Non-parametric' # notice how we created the new column \n",
    "                                                     # within the for loop -- we did not have \n",
    "                                                     # to do it beforehand\n",
    "for_2plot[50:60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now lets try the merged plot again, but this time we can make use of a simple seaborn command\n",
    "plt.close()\n",
    "sns.lmplot(x='rand1',y='rand2',hue='ranked?',data=for_2plot)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# That's a bit better, though there's not much to see there\n",
    "# Sometimes it is a better to plot non-normal relationships using a lowess curve. This is very\n",
    "# easy to do with our functions...:\n",
    "\n",
    "# lets first return our column to the way they were so we don't get confused...\n",
    "forplot_r.columns = ['ranked_rand1','ranked_rand2']\n",
    "\n",
    "plt.close()\n",
    "sns.lmplot('ranked_rand1','ranked_rand2',forplot_r,lowess=True)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The lowess curve revealed an interesting feature -- it seems there is a non-linear trend in\n",
    "# the data. Lets plot it the best quadratic fit:\n",
    "\n",
    "plt.close()\n",
    "sns.lmplot('ranked_rand1','ranked_rand2',forplot_r,order=2) # setting order to 2 for 2nd order\n",
    "                                                            # polynomial\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# In case you're interested how that line is generated, I'll show you! Its a multi-step process:\n",
    "\n",
    "# First we need to get the \"predicted\" quadratic curve. Numpy has a simple function that will \n",
    "# do the trick\n",
    "fit = np.polyfit(ranked_rand1,ranked_rand2,2) # 2 here refers that we are searching for  \n",
    "                                              # a fit to the data using a second order \n",
    "                                              # polynomial(i.e. parabolic relationship)\n",
    "    \n",
    "# now we save that into an equation we can use to transform y \n",
    "f = np.poly1d(fit) \n",
    "print(f,'\\n')\n",
    "\n",
    "# finally, we use apply that transform to get our \"new\" y\n",
    "y_new = f(ranked_rand1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot it for a sanity check...\n",
    "\n",
    "forplot_r['y_new'] = y_new # add 'y_new' to our mini-dataframe\n",
    "\n",
    "plt.close()\n",
    "fig, ax = plt.subplots() # initialize double-plot\n",
    "sns.regplot('ranked_rand1','ranked_rand2',forplot_r,order=2,ax=ax,color='r') # the original plot\n",
    "sns.regplot('ranked_rand1','y_new',forplot_r,order=2,ax=ax,color='b',marker=None) # now plot x against y_new\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we'll use still another method to check the fit of the model. This function performs linear\n",
    "# regression for a pair of variables. We get the same r,p values as pearsonr, but we have more\n",
    "# information, like standard error\n",
    "slope,inter,tst_r,tst_p,stderr = stat.linregress(ranked_rand1,ranked_rand2**2)\n",
    "print('r square is %s, p is %s, stderr is %s'%(tst_r**2,tst_p,stderr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Alas, looks can be decieving! Not even close to significant, despite the fancy quadratic \n",
    "# curve suggesting otherwise. Okay, fair enough, so maybe these \"random\" numbers are actually\n",
    "# random, at least with respect to one another. There are other things we could do to delve\n",
    "# further into this, but for now, let's move onto something else.\n",
    "\n",
    "# I'll start you off with some exercises to make sure you grasp the basic concepts of plotting\n",
    "# But first, let's take a break and get our dataframe in order. Right now, many of our\n",
    "# variables will be difficult to analyze. Let's fix that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ndf = deepcopy(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# PURIFY INTS IN NUMBER LIKE COLUMNS\n",
    "\n",
    "cols = ['How many romantic relationships have you been in that have lasted at least 6 months',\n",
    "        'How many siblings do you have?',\n",
    "        'How many cities have you lived in for at least six months',\n",
    "        'Assuming you stayed relatively healthy but aged normally, how old would you want to live to be?',\n",
    "        'Exactly how far in km is your commute to work?',\n",
    "        'What are the chances (in percentage from 1-100) that the Dutch National team will win a EuroCup or WorldCup championship in the next 10 years?',\n",
    "        'How many hours of sleep do you typically get on week days']\n",
    "        \n",
    "# Nothing fancy here. For each of the columns above, I'm using our self-made function to\n",
    "# identify problematic cells and fix them manually. Still, notice for each cell it only takes\n",
    "# me a few lines to do this. When you're comfortable with Python, it can be a lot faster than\n",
    "# excel. Notice however that I'm also changing the dtype of the columns using astype -- this is \n",
    "# just to make sure Pandas recognizes these columns are full of numbers.\n",
    "    \n",
    "us.purify_numbers(df[cols[0]])\n",
    "ndf.ix[25,cols[0]] = 2\n",
    "ndf.ix[49,cols[0]] = 6\n",
    "ndf[cols[0]] = us.purify_numbers(ndf[cols[0]],mode='apply').astype(float)\n",
    "print('\\n')\n",
    "\n",
    "us.purify_numbers(df[cols[1]])\n",
    "ndf.ix[36,cols[1]] = 1\n",
    "ndf[cols[1]] = ndf[cols[1]].astype(float)\n",
    "print('\\n')\n",
    "\n",
    "us.purify_numbers(df[cols[2]])\n",
    "ndf.ix[25,cols[2]] = 4\n",
    "ndf[cols[2]] = ndf[cols[2]].astype(float)\n",
    "print('\\n')\n",
    "\n",
    "us.purify_numbers(df[cols[3]])\n",
    "ndf.ix[[0,2,36],cols[3]] = 100\n",
    "ndf[cols[3]] = us.purify_numbers(ndf[cols[3]],mode='apply').astype(float)\n",
    "print('\\n')\n",
    "\n",
    "ndf[cols[4]] = df[cols[4]]\n",
    "bad_indz = us.purify_numbers(df[cols[4]])\n",
    "for ind in bad_indz:\n",
    "    val = ndf.ix[ind,cols[4]]\n",
    "    if 'k' in val:\n",
    "        val = val.split('k')[0]\n",
    "        if ',' in val:\n",
    "            val = val.replace(',','.')\n",
    "        ndf.ix[ind,cols[4]] = val\n",
    "ndf[cols[4]] = us.purify_numbers(ndf[cols[4]],mode='apply').astype(float)\n",
    "print('\\n')\n",
    "\n",
    "bad_indz = us.purify_numbers(df[cols[5]])\n",
    "for i in bad_indz:\n",
    "    ndf.ix[i,cols[5]] = ndf.ix[i,cols[5]]\n",
    "ndf[cols[5]] = us.purify_numbers(ndf[cols[5]],mode='apply').astype(float)\n",
    "print('\\n')\n",
    "\n",
    "bad_indz = us.purify_numbers(df[cols[6]])\n",
    "ndf.ix[[13,23,37],cols[6]] = 7.5\n",
    "ndf.ix[36,cols[6]] = 8.5\n",
    "ndf[cols[6]] = us.purify_numbers(ndf[cols[6]],mode='apply').astype(float)\n",
    "\n",
    "ndf['Pick another number from 1 to 100'] = rand2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CONVERT LISTED OUTPUT TO NUMBERS\n",
    "\n",
    "# For questions where users could select more than one answer, the survey outputted cells with\n",
    "# all answers where sep = ;. In one other column (languages), the users separated their\n",
    "# answers into columns. I will use one of our functions to take these cells and instead\n",
    "# output the number of answers or boxes they selected. Its not the coolest thing in the world\n",
    "# but its another variable to look at.\n",
    "\n",
    "# Because we need to do basically the same thing to each column, I can do this in only a few\n",
    "# lines. Note that its important, in this case, that the labels are in the same order as the \n",
    "# columns, or else the script would give us false outputs.\n",
    "\n",
    "labs = ['comp_exp','programmin_exp','img_software_exp','stats_exp',\n",
    "        'food_pickiness','drink_flexibility','sandwich_flexibility',\n",
    "       'languages']\n",
    "\n",
    "cols = ['Please select all of the following for which you have some experience', \n",
    "        'If you have programming experience, list languages to you are comfortable with.',\n",
    "        'If you have experience with image preprocessing software, please select which software ',\n",
    "        'If you have experience with statistical analysis, what are your preferred statistical softwares, engines or languages?',\n",
    "        'Do you strongly dislike the taste or texture of any of the following things?',\n",
    "        'Which of these do you regularly drink?',\n",
    "        'Which of these would you consider a sandwich?',\n",
    "        'List the languages you speak (conversationally)']\n",
    "\n",
    "for i,col in enumerate(cols): # For each of the columns above\n",
    "    for ind,entry in enumerate(ndf[col]): # For each entry (list of answers) in that column \n",
    "        if i == 7:            # if its the 7th column in the list...\n",
    "            sep = ','         # use a ',' as the delimiter\n",
    "        else:                 # otherwise\n",
    "            sep = ';'          # use a ';' as the delimiter\n",
    "        # separate the entry and return the number of items\n",
    "        val,z = us.handle_list(entry,sep=sep,len_only=True,verbose = False)\n",
    "        # make new column in ndf with output of handle_list, and call it by the ith label in labs\n",
    "        # P.S. that's why the labels need to be in the same order\n",
    "        ndf.ix[ind,labs[i]] = val \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CLEAN UP A FEW OTHER VARIABLES\n",
    "\n",
    "# Some other variable just need some specific TLC. I take care of them individually, usually\n",
    "# with the help of scripts we made\n",
    "\n",
    "# Get rid of this extra row\n",
    "ndf.drop(ndf.index[-1],axis=0,inplace=True)\n",
    "\n",
    "# Convert timestamp to time of day\n",
    "ndf['Time of day'] = us.time_of_day(ndf['Timestamp'])\n",
    "\n",
    "# convert keyboard mash to \"aggression\"\n",
    "ndf['aggression'] = us.character_count(\n",
    "        df['Fill this text box with gibberish by mashing random keyboard keys']).astype(float)\n",
    "\n",
    "# convert first five annimals to \"uniqueness\", based on how unique their animal choice was\n",
    "ndf['Uniqueness'],jnk = us.uniqueness(ndf['Name the first five animals you can think of'],\n",
    "                                     seq=True,seq_target=5,score_bins=10)\n",
    "ndf['Uniqueness'] = ndf['Uniqueness'].astype(float)\n",
    "\n",
    "# fix a few invalid entries in the Beer question and Dress question\n",
    "col = 'If you had to choose one, what would be your favorite type of beer?'\n",
    "for i,v in enumerate(df[col]):\n",
    "    if v == 'Amstel' or v == 'Tripel' or v == 'Triple, IPA, Pilsner':\n",
    "        print(i,v)\n",
    "ndf.ix[10,col] = 'Pilsner, Lager or Bock'\n",
    "ndf.ix[17,col] = 'Hefeweizen or Belgian'\n",
    "ndf.ix[17,col] = np.nan\n",
    "\n",
    "#col = df['What color is this dress? (If you\\'re familiar with this image, what color did you think it was when you first saw it)']\n",
    "#ndf.ix[[1,41],col] = 'Other' \n",
    "\n",
    "# deal with NaNs in two of the columns\n",
    "ndf['programmin_exp'] = us.handle_NaNs(ndf['programmin_exp'],mode='encode',code=0.0)\n",
    "ndf['food_pickiness'] = us.handle_NaNs(ndf['food_pickiness'],mode='encode',code=0.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Great, now we're ready for some exercises!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### EXERCISE 1 ####\n",
    "\n",
    "# Use this dataframe to call variables for all your analyses \n",
    "xdf = deepcopy(ndf)\n",
    "# If you accidentally mess up and use df or rdf, you can always go back and rerun the cell \n",
    "# where they were originally defined\n",
    "\n",
    "# Feel free to put the answers in different cells\n",
    "\n",
    "## PART A\n",
    "# Plot a histogram for the follwing variable:\n",
    "# 'Exactly how far in km is your commute to work?'\n",
    "# and determine (statistically) whether it is normally distributed\n",
    "# HINT: You'll have to somehow get rid of the NaNs!\n",
    "\n",
    "## PART B\n",
    "# Overlay KDE plots for the variables 'aggression' and 'uniqueness'. If there are outliers,\n",
    "# get rid of them first. Outliers should be defined as individuals values more than 3SDs away\n",
    "# from the mean\n",
    "\n",
    "## PART C\n",
    "# Determine if the following variables are normally distributed. \n",
    "# 'How many siblings do you have?','How good are you at sports?'\n",
    "# If so, run a pearson's correlation and plot the relationship with a normal regression line. \n",
    "# If not, run a spearman's correlation and plot the ranked data with a loess curve\n",
    "\n",
    "## PART D\n",
    "# Make an overlay plot showing the relationship 'drink_flexibility' has to BOTH \n",
    "# 'food_pickiness' and 'How spicy do you like your food?'. Z-score all columns before plotting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ANSWERS BELOW #\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Answers to exercises\n",
    "\n",
    "## PART A:\n",
    "# Plot a histogram for distance to work\n",
    "plt.close()\n",
    "col = xdf['Exactly how far in km is your commute to work?'].dropna()\n",
    "sns.distplot(col)\n",
    "plt.show()\n",
    "# Determine if its normally distributed\n",
    "print(stat.skewtest(col))\n",
    "print(stat.kurtosistest(col))\n",
    "print('This variable is NOT normally distributed')\n",
    "\n",
    "## PART B:\n",
    "# Get rid of outliers\n",
    "cols = ['aggression','Uniqueness']\n",
    "for col in cols:\n",
    "    cmn = xdf[col].mean()\n",
    "    csd = xdf[col].std()\n",
    "    for i,val in enumerate(xdf[col]):\n",
    "        if (val > cmn + (3*csd)) or (val < cmn - (3*csd)):\n",
    "            xdf.ix[i,col] = np.nan\n",
    "\n",
    "# Overlay KDE plots\n",
    "sns.kdeplot(xdf['aggression'])\n",
    "sns.kdeplot(xdf['Uniqueness'])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "## PART C:\n",
    "# Are variables normally distributed?\n",
    "cols = ['How many siblings do you have?','How good are you at sports?']\n",
    "print(stat.skewtest(xdf[cols[0]]))\n",
    "print(stat.kurtosistest(xdf[cols[0]]))\n",
    "print(stat.skewtest(xdf[cols[1]]))\n",
    "print(stat.kurtosistest(xdf[cols[1]]))\n",
    "print('siblings is not normally distributed \\n')\n",
    "\n",
    "# Run spearman\n",
    "print(stat.spearmanr(xdf[cols[0]],xdf[cols[1]].dropna()))\n",
    "\n",
    "# Rank data and make a df\n",
    "plt_df = pandas.DataFrame()\n",
    "plt_df['rank_sib'] = stat.rankdata(xdf[cols[0]])\n",
    "plt_df['rank_sports'] = stat.rankdata(xdf[cols[1]])\n",
    "\n",
    "# Plot with a loess curve\n",
    "sns.regplot('rank_sib','rank_sports',data=plt_df,lowess=True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "## PART D\n",
    "# Make plotting df\n",
    "cols = ['How spicy do you like your food?','food_pickiness','drink_flexibility']\n",
    "plt_df = deepcopy(xdf[cols])\n",
    "\n",
    "# zscore\n",
    "for col in cols:\n",
    "    mn,sd = plt_df[col].mean(),plt_df[col].std()\n",
    "    plt_df[col] = (plt_df[col]-mn)/sd\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots()\n",
    "sns.regplot('%s'%cols[-1],'%s'%cols[0],plt_df,ax=ax,color='b') \n",
    "ax2 = ax.twinx()\n",
    "sns.regplot('%s'%cols[-1],'%s'%cols[1],plt_df,ax=ax2,color='r') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### DEMONSTRATION 2:#\n",
    "#A CULTURAL EXPERIMENT ####\n",
    "\n",
    "'''Here, we will look at how individuals born in the Netherlands differ from those born \n",
    "elsewhere. We will go over simple t-tests, as well as chi-square test, and how to visualize\n",
    "such associations. We will also touch on multiple comparisons and permutation testing'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# I don't know about you, but I'm interested to know if people in the Netherlands that took the\n",
    "# survey differ at all from those not born in the Netherlands.\n",
    "\n",
    "# First, we still have lots of columns that can't be easily analyzed without some thoughtful\n",
    "# transformation. Let's isolate the testable variables into a separate list that we can refer\n",
    "# to whenever we want.\n",
    "\n",
    "# Since we're lazy, we'll try to think of a common thread that fits the variables we do or\n",
    "# don't want to analyze. It turns out that all of the variables we DONT want to analyze have\n",
    "# a dtype of \"object\" or \"O\" and have > 8 unique entries. Lets exploit that rule below\n",
    "\n",
    "tstcols = []\n",
    "for col in ndf.columns:\n",
    "    if ndf[col].dtype == 'O' and len(ndf[col].unique()) > 8:\n",
    "        continue\n",
    "    else:\n",
    "        tstcols.append(col)\n",
    "\n",
    "        \n",
    "# I'll only remove two others that still don't fit our criteria: \n",
    "tstcols.remove('stats_exp') # because all the values turned out to be the same\n",
    "tstcols.remove(tstcols[0]) # This is just the index\n",
    "\n",
    "# And here are the columns we're keeping!\n",
    "tstcols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To look at this, we will survey all of the variables in tstcols to see if there are any \n",
    "# associations. \n",
    "\n",
    "# PLEASE NOTE: This is <<NOT>> how to do science!! Data-mining and exploratory\n",
    "# analyses have their place in science, but what we're about to do is called a fishing \n",
    "# expedition and its GREATLY frowned upon because it often leads to the publication of results\n",
    "# that are not replicable, therefore often wasting precious time and energy for the scientific\n",
    "# community.\n",
    "\n",
    "# The reason we are doing it here is just to find some significant associations to showcase\n",
    "# some of the statistical and visualization tools Python has to offer. However, just to be on\n",
    "# the right side of things, we will be sure to correct our results for multiple comparisons.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now that that disclaimer is out of the way, I'll unveil a little script I wrote to find\n",
    "# significant associations in a quick and very dirty way. You can read the documentation to\n",
    "# understand what it does, and look at the source code in pycourse_lesson3_4_scripts if you\n",
    "# want to know how it works. You can use this script for your project, but please use it and \n",
    "# other such methods wisely for applications outside of this class.\n",
    "\n",
    "NL = 'Were you born in the Netherlands?'\n",
    "\n",
    "# This script will notify us about significant relationships between the column above and other\n",
    "# variables in our dataset\n",
    "NL_ps = us.data_miner(xcols=[NL],ycols=tstcols,data=ndf,return_ps=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Very interesting!! It looks like there's a relationship between being from the\n",
    "# Netherlands and traveling! \n",
    "\n",
    "# Those are some nice p-values, but they may not be enough to survive multiple comparisons \n",
    "# tests, since we tested so many variables. The most conservative method for correcting for\n",
    "# multiple comparisons is the Bonferonni method, where you just divide your alpha level \n",
    "# (often 0.05) by the number of comparisons. Lets do that here\n",
    "\n",
    "0.05/len(tstcols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Its plain to see that none of our significant relationships survive that threshold, but if\n",
    "# that was not so clear, we could always check. \n",
    "\n",
    "# The p-values for each relationship has been stored in a DataFrame called NL_ps. \n",
    "\n",
    "print(NL_ps.head(),'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lets search it just to make sure\n",
    "\n",
    "for relationship,row in NL_ps.iterrows():\n",
    "    if row[0] < (0.05/len(tstcols)):\n",
    "        print(relationship)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Nothing. However, there are other less severe methods for multiple comparisons corrections.\n",
    "# The false discovery rate (FDR) is a more relaxed method that lets researchers \"tolerate\" some\n",
    "# False discoveries, as long as they can control the expected rate. Its also great for\n",
    "# non-independent sets of tests (not the case here, but often the case in neuroimaging).\n",
    "\n",
    "# See here for a short, fairly simple and very nice explanation: \n",
    "# https://brainder.org/2011/09/05/fdr-corrected-fdr-adjusted-p-values/\n",
    "\n",
    "# It would be fun to do FDR by hand, but we can save that for another day. Luckily, the \n",
    "# statsmodels library has an FDR correction function, though its a bit hard to find. Lets \n",
    "# import it directly into our namespace and look at its docstring.\n",
    "\n",
    "from statsmodels.sandbox.stats.multicomp import fdrcorrection0 as fdr\n",
    "fdr?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Lets enter all of our pvalues into the FDR function with a Q of 0.1 and see what we get...\n",
    "\n",
    "# Get rid of NaNs (fdr can't handle them) and convert to a list\n",
    "pvals = NL_ps['p'].dropna().tolist()\n",
    "\n",
    "NL_fdr = fdr(pvals,alpha=0.1,method='indep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now we'll print the output. The first array list True for any variable that survives FDR \n",
    "# correction at this alpha level (Q), while the second array lists what Q would need to be set \n",
    "# to for the variable to survive FDR\n",
    "NL_fdr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sorry folks, our sample just isn't big enough for these relationships to be taken at face\n",
    "# value. Let's take a look at some of the trending relationships anyway, but just keep in mind\n",
    "# the data is not reliable.\n",
    "\n",
    "# By the way, you can get the same functionality as above with Bonferonni (FWE) correction\n",
    "# using the FWE method:\n",
    "\n",
    "# from statsmodels.sandbox.stats.multicomp.multipletests\n",
    "\n",
    "# Other methods can be found there as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# It seems the Dutch have some interesting relationships with travel! \n",
    "# Let's have a look at that. \n",
    "\n",
    "# First, lets make a mini-df of only the variables we care about\n",
    "\n",
    "adf = deepcopy(ndf)\n",
    "x = 'Were you born in the Netherlands?'\n",
    "y1 = 'How many continents have you visited'\n",
    "y2 = 'How many cities have you lived in for at least six months'\n",
    "y3 = 'What style of music do you most prefer?'\n",
    "\n",
    "NL_df = pandas.concat([adf[x],adf[y1],adf[y2],adf[y3]],axis=1)\n",
    "NL_df.head()\n",
    "                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now, lets make sure pandas knows which of these variables are \"categorical\"\n",
    "NL_df[x] = NL_df[x].astype('category')\n",
    "NL_df[y3] = NL_df[y3].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# After those simple and fairly painless steps, the data can be easily explored and plotted.\n",
    "# Lets make a few pivot tables so we can look at the means visually.\n",
    "print(NL_df.pivot_table(y1,x))\n",
    "print('\\n')\n",
    "print(NL_df.pivot_table(y2,x))\n",
    "print('\\n')\n",
    "\n",
    "# For two categorical variables, we'll use the crosstab function\n",
    "pandas.crosstab(NL_df[x],NL_df[y3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Don't worry, we'll come back to the music stuff. Lets stick to the other two variables\n",
    "# for a moment. It seems like the Dutch are more likely to have visited multiple continents,\n",
    "# but tend to live in less cities.\n",
    "\n",
    "# Let's plot these relationships -- again, very easy now that the data is \n",
    "# prepared the way we need\n",
    "\n",
    "plt.close()\n",
    "sns.swarmplot(x,y1,data=NL_df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# I like this type of plot because you can easily see all of the datapoints, so the n of each\n",
    "# group is clear. Some might prefer a similar plot, called a violin plot, which is a bit\n",
    "# easier to see the nature of a relationship but also the variable's distribution\n",
    "\n",
    "plt.close()\n",
    "sns.violinplot(x, y1, data=NL_df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Very pretty! They look like stingrays! (pijlstaartrog?)\n",
    "# We can also combine these two plots (we'll remove the inner part of the violin plot so we\n",
    "# can see the points better), and we'll change the color of the points as well\n",
    "\n",
    "plt.close()\n",
    "sns.violinplot(x, y1, data=NL_df,inner=None)\n",
    "sns.swarmplot(x,y1,data=NL_df,color='w')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# These types of plots can be useful if you want to visualize a three-way interaction with\n",
    "# two categorical variables. You can add a third variable represented by color:\n",
    "\n",
    "plt.close()\n",
    "sns.swarmplot(x,y1,data=NL_df,hue=y3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Whoops, the labels are all messed up. Not a big deal, its not so hard to control the plots.\n",
    "# We *could* actually control every aspect of those labels, including their position and size.\n",
    "# For now, lets settle for making the y-axis bigger -- its a crappy work around but we have \n",
    "# other fish to fry\n",
    "\n",
    "sns.swarmplot(x,y1,data=NL_df,hue=y3)\n",
    "sns.plt.ylim([0,10]) # change the limits of y-axis to 0 and 10\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# If we wanted to get *really* fancy, and we have, for example, a three-way relationship\n",
    "# between two binary variables and a third scalar variable, we could do this to better\n",
    "# visualize the distributions:\n",
    "\n",
    "# Add another binary variable\n",
    "z = 'Do you ever eat/drink condiments straight from the bottle/can?'\n",
    "NL_df[z] = ndf[z].astype('category')\n",
    "\n",
    "# Plot \n",
    "plt.close()\n",
    "sns.violinplot(x, y1, data=NL_df,hue=z,split=True)\n",
    "sns.plt.ylim([-2,10])\n",
    "plt.show()\n",
    "\n",
    "# By the way, if you don't like these colors, you can mess around with them by setting the\n",
    "# palette argument to a different set, like palette = 'Set3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Okay, now lets have a look at barplots. Let's look at the other variable instead. Again, its\n",
    "# very easy with seaborn..\n",
    "\n",
    "sns.barplot(x,y2,data=NL_df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Cool! Lets change the palette to something a bit nicer, make the graph a bit smaller,\n",
    "# and lets change the background to a different style. \n",
    "\n",
    "sns.set_style(\"white\")\n",
    "sns.plt.figure(figsize=(6,5))\n",
    "sns.barplot(x,y2,data=NL_df,palette='Greens_d')\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Lets look at our two plots together\n",
    "plt.close()\n",
    "bothplots = sns.PairGrid(NL_df,\n",
    "                 x_vars=[x],\n",
    "                 y_vars=[y1,y2],\n",
    "                 aspect=.75, size=3.5)\n",
    "bothplots.map(sns.barplot, palette=\"Greens_d\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Again, its easy to add multiple variables to the boxplots with the hue argument:\n",
    "plt.close()\n",
    "sns.barplot(x,y2,data=NL_df,hue=z,palette=\"Set2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Okay, enough visualization. Lets look at how to test some of these relationships.\n",
    "# The scipy.stats library has many simple-to-run statistical tests, so of course it has a\n",
    "# t-test function as well. Actually -- it has four different t-tests. We want to a run an \n",
    "# independent samples t-test in this case\n",
    "\n",
    "stat.ttest_ind?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# In order to do this comfortably, we need to know whether there is a legitimate equality of\n",
    "# variances between our populations. If not, we'll want to use the Welch's t-test as the \n",
    "# docstring explains. A simple google search reveals that a Levene's and Bartlett's tests \n",
    "# exist for this purpose. Sure enough, that stats module contains both. A Levene's test is more\n",
    "# appropriate in the case of non-normal distributions. How normal are our variables?\n",
    "\n",
    "print(stat.skewtest(NL_df[y1]))\n",
    "print(stat.kurtosistest(NL_df[y1]))\n",
    "print(stat.skewtest(NL_df[y2]))\n",
    "print(stat.kurtosistest(NL_df[y1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Looks like the distributions are normal. Let's use a Bartlett's test then.\n",
    "\n",
    "stat.bartlett?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Notice the input arguments are not exactly set up in a way that is easy for us. Arguments a \n",
    "# and b represent the two \"populations\" we are studying. That means we need to input separate \n",
    "# arrays for subjects who were born in the Netherlands, and subjects who were not. \n",
    "# Unfortunately, t-tests are set up this way as well.\n",
    "\n",
    "# Lucky for us, we know how to use pandas for querying, which will make this very easy\n",
    "# For example, here, I'll grab only rows for columns where born in Netherlands is Yes, and\n",
    "# from that, I'll take only the column corresponding to # of continents lived in\n",
    "NL_df[NL_df[x] == 'Yes'][y1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Easy! Now lets set our variables:\n",
    "a1 = NL_df[NL_df[x] == 'Yes'][y1]\n",
    "b1 = NL_df[NL_df[x] == 'No'][y1]\n",
    "\n",
    "a2 = NL_df[NL_df[x] == 'Yes'][y2]\n",
    "b2 = NL_df[NL_df[x] == 'No'][y2]\n",
    "\n",
    "# And run the test to check homogeneity of variances...\n",
    "print(stat.bartlett(a1,b1))\n",
    "print(stat.bartlett(a2,b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Looking good. Let's run the ttests then (no need for the Welch's!)\n",
    "print(stat.ttest_ind(a1,b1))\n",
    "print(stat.ttest_ind(a2,b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Lets go back and have a look at our categorical variable where we found the other\n",
    "# significant association. We'll use the pandas.crosstab function to get a better look at the\n",
    "# data\n",
    "\n",
    "NL_ct = pandas.crosstab(NL_df[x],NL_df[y3])\n",
    "NL_ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Its a bit hard to interpret this as sums. Lets convert it to percentages\n",
    "# The quickest way to do this is transpose the crosstabs and calculate the percentages by hand\n",
    "\n",
    "t_NL_ct = NL_ct.transpose()\n",
    "colsum=t_NL_ct.sum(axis=0)\n",
    "perc_NL_ct=t_NL_ct/colsum\n",
    "perc_NL_ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We can already sort of see the differences in the distributions. NLers have a pretty eclectic\n",
    "# taste with the most popular music being, well, pop music. Others seem to be mostly into Rock,\n",
    "# Hip-Hop and, particularly, Alternative/Indie.\n",
    "\n",
    "# We can already conclude from this that the Dutch tend to have worse taste in music, but lets \n",
    "# first test this association for statistical significance. We can use a chi-square test for \n",
    "# this purpose\n",
    "\n",
    "stat.chisquare?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Running it is pretty easy. But the test won't take string as input. Instead, we'll need to\n",
    "# give the categories integer values. Luckily we built a function to do this.\n",
    "\n",
    "x_enc = us.encode(NL_df[x].astype('O')) # this won't work with an object of type \"category\"\n",
    "y3_enc = us.encode(NL_df[y3].astype('O')) # hence the astype('O')\n",
    "x_enc.head(),y3_enc.head()\n",
    "\n",
    "\n",
    "#stat.chisquare(NL_df[x],NL_df[y3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# And now we run the test\n",
    "stat.chisquare(x_enc,y3_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Visualizing such relationships can be tricky. Pandas has a built-in plot function that\n",
    "# does it. We can print directly from our crosstab we created\n",
    "\n",
    "plt.close()\n",
    "NL_ct.plot.bar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This will be much easier to interpret with percentages. Let's take the percentage crosstabs\n",
    "# we made before, transpose it, and plot it in the same manner\n",
    "plt.close()\n",
    "perc_NL_ct.transpose().plot.bar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# One final (and I think super cool) way to visualize these relationships is to just look at\n",
    "# the raw distributions overlaid over one another. We'll use the KDE plots from before to do\n",
    "# this, and we'll use the encoded versions of the variables.\n",
    "\n",
    "# We'll also have to wrangle the data in a similar way that we did with the t-tests, since we\n",
    "# want to look at the distributions of the NL+ and NL- separately.\n",
    "\n",
    "NL_df['music_encoded'] = y3_enc\n",
    "a3 = NL_df[NL_df[x]=='Yes']['music_encoded']\n",
    "b3 = NL_df[NL_df[x]=='No']['music_encoded']\n",
    "\n",
    "plt.close()\n",
    "sns.kdeplot(a3)\n",
    "sns.kdeplot(b3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Frankly, I'm a bit surprised this is significant. I wonder how many times such a significant\n",
    "# relationship would occur by chance if we kept the number of Dutch and non-Dutch the same.\n",
    "# We can test this easily in Python using permutation. Here, we'll combine familiar Python \n",
    "# syntax with our newfangled statistical abilities.\n",
    "\n",
    "# In other words, we can randomly shuffle our x value 10,000 times, but keeping the number of\n",
    "# \"Yes\" and \"No\" values the same. Then we can  test the presence of a relationship with \n",
    "# music preference in each of these random samples, and see the probability that the\n",
    "# relationship we observed would happen just out of sheer chance!\n",
    "\n",
    "# All we need to do create a for loop where, for each iteration, we generate a random \n",
    "# permutation of our x data, and then perform the a chi square test and store the outputs\n",
    "\n",
    "tst_chi,tst_p = stat.chisquare(x_enc,y3_enc)\n",
    "\n",
    "null_dist = []\n",
    "for i in range(10000):\n",
    "    rsamp = np.random.permutation(x_enc)\n",
    "    chi,p = stat.chisquare(rsamp,y3_enc)\n",
    "    null_dist.append(chi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now we can see where our test value falls within the null distribution...\n",
    "# There are many ways to do this. This isnt a very efficient way but its easy to follow.\n",
    "# For each value (chi-square) in the null distribution, if the value is lower \n",
    "# (i.e. \"stronger\") than our observed test statistic, we store it. At the end, we find the \n",
    "# p-value by finding the ratio of values greater than our test statistic vs. the total \n",
    "# sample of 10000 tests.\n",
    "\n",
    "cont = []\n",
    "for r_chi in sorted(null_dist):\n",
    "    if r_chi < tst_chi:\n",
    "        cont.append(r_chi)\n",
    "exact_p = len(cont)/float((len(null_dist)+1))\n",
    "print(exact_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now that we're testing against a \"true\" null distribution, the association has fallen to\n",
    "# trend levels. Lets visualize this!\n",
    "\n",
    "# First we'll plot our null distribution\n",
    "plt.close()\n",
    "sns.kdeplot(np.array(null_dist))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A beautiful, normal null distribution! Now lets look at where in the distribution a chi\n",
    "# statistic would have to fall to be considered significant in a one-sided test\n",
    "\n",
    "# Find out the chi stat at 0.05:\n",
    "sig_chi = sorted(null_dist)[-9500]\n",
    "\n",
    "# Plot it over the distribution\n",
    "sns.kdeplot(np.array(null_dist))\n",
    "sns.plt.axvline(sig_chi,color='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# And finally, lets add our actual observed test_statistic:\n",
    "\n",
    "plt.close()\n",
    "sns.kdeplot(np.array(null_dist))\n",
    "sns.plt.axvline(sig_chi,color='r')\n",
    "sns.plt.axvline(tst_chi,color='k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I guess we don't have statistical proof that the Dutch have bad taste in music ;-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### EXERCISES 2 ######\n",
    "\n",
    "# Once again, please call variables from this df and feel free to put your answers in different\n",
    "# cells\n",
    "xdf = deepcopy(ndf)\n",
    "\n",
    "## PART A\n",
    "# Do people avoid travel if they have a bad sense of direction?\n",
    "# Perform a t-test comparing these two variables and plot the results with a barplot\n",
    "# 'How many continents have you visited'\n",
    "# 'Would you say you have a good sense of direction?'\n",
    "\n",
    "## PART B\n",
    "# You want to know the relationship between music preference and whether people go to bed later\n",
    "# than planned.\n",
    "# Create a crosstab of percentages to look at the relationships. Then, run a chi-square test\n",
    "# and plot the results (in percentages) with your plot of choice\n",
    "\n",
    "## PART C\n",
    "# Though we didn't touch on it, their is an one-way ANOVA function in scipy.stats. \n",
    "# You can find it in stat.f_oneway. The concept is very similar to a t-test so you should be\n",
    "# able to figure it out.\n",
    "# You've heard that the oxford comma is a serious controversy in grammar, but why are people\n",
    "# so split about it? Use a one-way ANOVA to determine the relationship between these two\n",
    "# variables:\n",
    "# 'What are your thoughts on the oxford comma?'\n",
    "# 'How many romantic relationships have you been in that have lasted at least 6 months'\n",
    "# Then, plot the results. Make the plot prettier by changing the labels for the oxford comma\n",
    "# question\n",
    "\n",
    "## PART D\n",
    "# Write a t-test function. The function should accept a grouping variable, a scalar variable,\n",
    "# and a dataframe. The variables should be passed as string (column names)\n",
    "# The function should do the following:\n",
    "# 1) Set up the variables to be entered into a t-test\n",
    "# 2) Determine if the non-grouping variable is normally distributed\n",
    "# 3) If so, perform a Bartlett's test\n",
    "# 4) Depending on the outcome of 2), run a regular, or Welch's t-test\n",
    "# 5) If 1) if False, instead perform a Mann-Whitney U test\n",
    "\n",
    "# BONUS: Make the script raise appropriate errors for bad inputs\n",
    "\n",
    "# Once you've written the function, test it, iteratively, on the following pairs of variables:\n",
    "\n",
    "# Have you ever practiced any non-allergy based dietary restrictions (e.g. vegetarian, gluten-free, vegan, etc)    vs   \n",
    "# Assuming you stayed relatively healthy but aged normally, how old would you want to live to be? 2.18191970962 tf 0.0344995536103 *\n",
    "\n",
    "# Were you born in the Netherlands? vs\n",
    "# What are the chances (in percentage from 1-100) that the Dutch National team will win a EuroCup or WorldCup championship in the next 10 years?\n",
    "\n",
    "# Do you feel sleepy a lot?    vs   \n",
    "# Exactly how far in km is your commute to work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ANSWERS BELOW #\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### ANSWERS TO EXERCISES II ####\n",
    "\n",
    "xdf = deepcopy(ndf)\n",
    "## PART A\n",
    "# Run t-test\n",
    "cols = ['How many continents have you visited',\n",
    "        'Would you say you have a good sense of direction?']\n",
    "a = xdf[xdf[cols[1]] == 'Yes'][cols[0]]\n",
    "b = xdf[xdf[cols[1]] == 'No'][cols[0]]\n",
    "print(stat.ttest_ind(a,b))\n",
    "\n",
    "# plot\n",
    "plt.close()\n",
    "sns.barplot(cols[1],cols[0],data=xdf,palette='Blues_d')\n",
    "plt.show()\n",
    "\n",
    "## PART B\n",
    "# Create a crosstab of percentages to look at the relationships. \n",
    "x = 'What style of music do you most prefer?'\n",
    "y = 'Do you often go to bed later than you wanted to?'\n",
    "ct = pandas.crosstab(xdf[y],xdf[x])\n",
    "t_ct = ct.transpose()\n",
    "colsum=t_ct.sum(axis=0)\n",
    "perc_ct=t_ct/colsum\n",
    "print(perc_ct)\n",
    "\n",
    "# Then, run a chi-square test\n",
    "stat.chisquare(us.encode(xdf[x]),us.encode(xdf[y]))\n",
    "\n",
    "# and plot the results (in percentages) \n",
    "perc_ct.transpose().plot.bar()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "### SEE NEXT CELL FOR ANSWERS TO C AND D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## PART C\n",
    "# Use a one-way ANOVA to determine the relationship \n",
    "x = 'What are your thoughts on the oxford comma?'\n",
    "y = 'How many romantic relationships have you been in that have lasted at least 6 months'\n",
    "\n",
    "plt_df = deepcopy(xdf[[x,y]])\n",
    "gps = plt_df[x].unique()\n",
    "pops = []\n",
    "for g in gps:\n",
    "    if pandas.notnull(g):\n",
    "        g_dat = np.array(plt_df[plt_df[x] == g][y].dropna())\n",
    "        pops.append(g_dat)\n",
    "print(stat.f_oneway(pops[0],pops[1],pops[2]))\n",
    "\n",
    "# fix labels\n",
    "newlabs = {'I use the oxford comma':'Love it',\n",
    "          'I most certainly do NOT use the oxford comma': 'Hate it',\n",
    "          'WTF is the oxford comma?': 'Huh?'}\n",
    "new_x = 'Thougts on oxford comma'\n",
    "plt_df[new_x] = us.encode(plt_df[x],newlabs)\n",
    "sns.barplot(new_x,y,data=plt_df)\n",
    "plt.show()\n",
    "        \n",
    "\n",
    "## PART D\n",
    "# Write a t-test function. The function should accept a grouping variable, a scalar variable,\n",
    "# and a dataframe. \n",
    "\n",
    "def jake_ttest(x,grp_var,data):\n",
    "    \n",
    "    # 1) Set up the variables to be entered into a t-test\n",
    "    if type(x) != str or type(grp_var) != str:\n",
    "        raise ValueError('input variables must be strings representing column labels in data')\n",
    "    if x not in data.columns or grp_var not in data.columns:\n",
    "        raise ValueError('x and grp_var must correspond to column labels in data')\n",
    "    \n",
    "    grps = data[grp_var].dropna().unique()\n",
    "    \n",
    "    if len(grps) > 2:\n",
    "        raise ValueError('grp_var must be a binary variable')\n",
    "    \n",
    "    try:\n",
    "        pandas.to_numeric(data[x])\n",
    "    except:\n",
    "        raise TypeError('column corresponding to argument x must be of a number type')\n",
    "    \n",
    "    a = data[data[grp_var] == grps[0]][x].dropna()\n",
    "    b = data[data[grp_var] == grps[1]][x].dropna()\n",
    "    \n",
    "# 2) Determine if the non-grouping variable is normally distributed\n",
    "    normal = True\n",
    "    s,p1 = stat.skewtest(data[x])\n",
    "    s,p2 = stat.kurtosistest(data[x])\n",
    "    if p1 < 0.05 or p2 > 0.05:\n",
    "        normal = False\n",
    "    \n",
    "    if normal:\n",
    "        # 3) If so, perform a Bartlett's test\n",
    "        print('x is normally disributed')\n",
    "        homo_vars = True\n",
    "        s,bp = stat.bartlett(a,b)\n",
    "        # 4) Depending on the outcome of 2), run a regular, or Welch's t-test\n",
    "        if bp < 0.05:\n",
    "            print('homogeneity of variances demonstrated. Running ttest...')\n",
    "            print(stat.ttest_ind(a,b))\n",
    "        else:\n",
    "            print('imhomogenous variances detected, running Welch\\'s ttest...')\n",
    "            print(stat.ttest_ind(a,b,equal_var=False))\n",
    "    else:\n",
    "        # 5) If 1) if False, instead perform a Mann-Whitney U test\n",
    "        print('x is not normally distributed, running Mann-Whitney U test...')\n",
    "        print(stat.mannwhitneyu(a,b,alternative='two-sided'))\n",
    "\n",
    "cols = ['Have you ever practiced any non-allergy based dietary restrictions (e.g. vegetarian, gluten-free, vegan, etc)',\n",
    "        'Assuming you stayed relatively healthy but aged normally, how old would you want to live to be?',\n",
    "        'Were you born in the Netherlands?',\n",
    "        'What are the chances (in percentage from 1-100) that the Dutch National team will win a EuroCup or WorldCup championship in the next 10 years?',\n",
    "        'Do you feel sleepy a lot?',\n",
    "        'Exactly how far in km is your commute to work?']\n",
    "col_pairs = {cols[0]:cols[1],\n",
    "            cols[2]:cols[3],\n",
    "            cols[4]:cols[5]}\n",
    "for gv,x in col_pairs.items():\n",
    "    jake_ttest(x,gv,xdf)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Once you've written the function, test it, iteratively, on the following pairs of variables:\n",
    "\n",
    "# Have you ever practiced any non-allergy based dietary restrictions (e.g. vegetarian, gluten-free, vegan, etc)    vs   \n",
    "# Assuming you stayed relatively healthy but aged normally, how old would you want to live to be? 2.18191970962 tf 0.0344995536103 *\n",
    "\n",
    "# Were you born in the Netherlands? vs\n",
    "# What are the chances (in percentage from 1-100) that the Dutch National team will win a EuroCup or WorldCup championship in the next 10 years?\n",
    "\n",
    "# Do you feel sleepy a lot?    vs   \n",
    "# Exactly how far in km is your commute to work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### DEMONSTRATION 3: AN INTERACTIVE CULTURAL EXPERIMENT ###   \n",
    "\n",
    "'''We will continue to examine the strange ways of the Dutch, but this time we'll look\n",
    "at some multivariate linear models'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Until now, I've shown you some pretty nice visualization techniques and a few nice tools like\n",
    "# FDR correction and permutation. However, the stats I've shown you have been quite basic, and\n",
    "# probably unhelpful for all but the simplest of statistical scenarios. Scipy.stats has many\n",
    "# other higher level tools, but ultimately, you're going to want something with more thorough\n",
    "# output and greater control if you're going to use Python for more in-depth analyses. \n",
    "\n",
    "# Statsmodels is a library that fits this profile. Its has a very impressive arsenal of\n",
    "# statistical tests, including everything scipy has and many more specific and modern \n",
    "# techniques. Also, it has a function that lets it behave almost exactly like R, with identical\n",
    "# output and ostensibly similar implementation.\n",
    "\n",
    "# Lets have a look at this implementation so folks coming from R can feel a bit more\n",
    "# comfortable\n",
    "\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For those not familiar with r, the formula api from statsmodels lets users enter an actual\n",
    "# equation (in a certain format), such as a regression equation, and fit a model based on the \n",
    "# equation. The equation is written in string, and will work under the following\n",
    "# circumstances:\n",
    "# 1) The equation makes sense\n",
    "# 2) Variables in the equation correspond directly to column names in your dataframe\n",
    "# 3) Those variables/column names are understandable to the function -- so for example, it gets\n",
    "#    confused by ?s\n",
    "# 4) The columns of your dataframe that are called by the equation may need to be of a number \n",
    "#    dtype, depending on their position in the equation.\n",
    "\n",
    "# Fulfill those conditions, and making for example a GLM or mixed model is quite easy. And\n",
    "# the output is fantastic.\n",
    "\n",
    "# Let's go back to investigating the Dutch using some multivariate GLMs. We'll use an ordinary\n",
    "# least squares procedure that is nearly identical to Rs \"lm\" function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rdf = deepcopy(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# First, we'll use the statsmodel formula api to reproduce an association we've already seen. \n",
    "# To do that, lets set up a dataframe of our variables of interest and make sure everything is \n",
    "# set so that the formula won't error\n",
    "\n",
    "# First, lets gather our variables into a dataframe\n",
    "cols = ['Were you born in the Netherlands?',\n",
    "        'How spicy do you like your food?',\n",
    "        'How many continents have you visited',\n",
    "        'How good are you at sports?',\n",
    "        'How many romantic relationships have you been in that have lasted at least 6 months',\n",
    "        'languages']\n",
    "\n",
    "mv_df = deepcopy(ndf[cols])\n",
    "mv_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now lets check our dtypes to make sure they're all number types\n",
    "mv_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Only the NL question is of the object type. In this case, we don't need to, but let's fix \n",
    "# this anyway by creating a new columns where this variable is encoded\n",
    "mv_df['NL_encoded'] = us.encode(mv_df[mv_df.columns[0]]).astype(float)\n",
    "\n",
    "mv_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Finally, lets change our columns to be more equation-friendly\n",
    "newcols = ['NLer','spiciness','continents','sports',\n",
    "           'relationships','languages','NL_encoded']\n",
    "mv_df.columns = newcols\n",
    "mv_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now we're ready to do some analysis. Let's replicate an effect we found before using \n",
    "# statsmodels\n",
    "\n",
    "# We'll use the ols function, which stands for ordinary least squares. Statsmodels has many\n",
    "# other functions that work similarly to R, such as Logits, mixed models, and so forth. You can\n",
    "# explore then if you want. We'll keep it simple for now.\n",
    "\n",
    "model1 = smf.ols('continents ~ NLer',data=mv_df).fit()\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pretty impressive output isn't it! Here we have a lot more useful information.\n",
    "# You'll also notice that the p-value for the whole model, which is listed as \n",
    "# Prob (F-statistic)is identical to the results of the t-test we ran previously.\n",
    "\n",
    "# Whats also really cool is that this model variable we created now contains tons of \n",
    "# information in it that can be used for other things. Have a look:\n",
    "\n",
    "# model1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This can be useful if you just want to extract, for example, the paramaters or the\n",
    "# residuals\n",
    "m1_params = model1.params\n",
    "m1_resids = model1.resid\n",
    "print(m1_params)\n",
    "print('\\n')\n",
    "print(m1_resids.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# You'll notice that these outputs are familiar. Have a look at the type!\n",
    "type(m1_resids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# They are pandas Series. This makes it super easy to, for example, append the residuals of a\n",
    "# relationship to an existing dataframe\n",
    "\n",
    "# Okay, lets expand this a bit and add another variable. How much you traveled might influence\n",
    "# the food you eat, so lets have a look at spiciness and expand the equation into multivariate\n",
    "# model\n",
    "\n",
    "model2 = smf.ols('continents ~ NLer + spiciness',data=mv_df).fit()\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Well, that didn't seem to add much. Being from the Netherlands still significantly impacts \n",
    "# continental travel history, independently of spicy food-seeking behavior (lord knows there's\n",
    "# not much spicy food in Europe/North America -- except Mexico). Our model fit actually\n",
    "# decreased, maybe that hypothesis was unfounded.\n",
    "\n",
    "# But for the sake of showing off statsmodels, I'll show you how to run an interaction. Its \n",
    "# simple:\n",
    "\n",
    "model3 = smf.ols('continents ~ NLer * spiciness',data=mv_df).fit()\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A significant interaction has occured!! Notice how, since we plotted an interaction, the main\n",
    "# effects were automatically included. Lets have a look at that interaction\n",
    "\n",
    "plt.close()\n",
    "sns.lmplot('continents','spiciness',hue='NLer',data=mv_df,palette='Set2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Wow! Apparently Dutch people who have visited more continents like spicy food more, (or vice\n",
    "# versa), but that logic doesn't seem to apply to folks born outside NL!\n",
    "\n",
    "# Just for the heck of it, lets use a different statsmodel function to compare model fit of\n",
    "# these two models\n",
    "\n",
    "from statsmodels.stats.anova import anova_lm\n",
    "\n",
    "anova_lm(model1,model3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Well.. there are a bunch of inauspicious warnings, but... if we ignore them, we see a\n",
    "# significant change in model fit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Lets look at another example. They say being good at sports gets you the gals/girls. Lets\n",
    "# see if that holds true\n",
    "smf.ols('relationships ~ sports',data=mv_df).fit().summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Maybe if we had a bigger sample size it would be significant, but for now it appears to be\n",
    "# a trend. But what if the problem is that we're sampling from two different distributions.\n",
    "# Maybe there's some sort of confound in here -- perhaps NLers are both more likely to say\n",
    "# they're good at sports AND also more likely to have been in more relationships. Lets enter\n",
    "# NLer status into the model and see what happens.\n",
    "\n",
    "smf.ols('relationships ~ sports + NLer',data=mv_df).fit().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now the effect of self-described sportiness is significantly associated with number of \n",
    "# relationships! But if we want to plot it, we'll need the residuals. Luckily, the integration\n",
    "# between statsmodels and seaborn through pandas makes this all pretty painless.\n",
    "\n",
    "# first run the models to get the residuals and put them into our dataframe\n",
    "\n",
    "mv_df['relationships_res'] = smf.ols('relationships ~ NLer',data=mv_df).fit().resid\n",
    "mv_df['sports_res'] = smf.ols('sports ~ NLer',data=mv_df).fit().resid\n",
    "\n",
    "# and now we plot\n",
    "plt.close()\n",
    "sns.lmplot('relationships_res','sports_res',data=mv_df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# As usual, there is SOOOO much more I could show you -- we're barely scratching the surface\n",
    "# here. The point is, if there is some sort of statistical procedure you'd like to perform,\n",
    "# chances are very high that python has a nice implementation of it, and a way for you to plot\n",
    "# it. \n",
    "\n",
    "# The online documentation and tutorials for seaborn is excellent. With statsmodels.. its,\n",
    "# okay, but it least documentation exists can be referenced. Scipy has a great deal of \n",
    "# functionality that extends far beyond basic stats, and a huge user-base online. Now that you\n",
    "# know some of the basics, it shouldn't be hard to get your feet wet doing what it is you want\n",
    "# to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############# ASSIGNMENT #############\n",
    "\n",
    "# You now have what it takes to do basic data-analysis with Python. For your next assignment,\n",
    "# I want you to do just that: analyze some data!\n",
    "\n",
    "# You can either use our data (just write ndf to a csv and load it in another new notebook) or\n",
    "# your own data. Either way, I want you to perform a few related analyses. It doesn't matter\n",
    "# if they are significant or not, but what's important is that you run the stats and plot the\n",
    "# relationships.\n",
    "\n",
    "# I would like AT LEAST three separate but related analyses. The analyses should tell a story \n",
    "# and/or should have a clear direction/narrative. If you can't do that, at the very least, try \n",
    "# to use three DIFFERENT types of analyses. Put the whole thing inside of a Jupyter notebook \n",
    "# and send it to me.\n",
    "\n",
    "# I also ask that you please email me to tell me a very brief/vague idea of what you are \n",
    "# analyzing (I want to be sure each student pursues a different question). First come first\n",
    "# serve so be sure to email me as you know. If you're having trouble coming up with a topic, \n",
    "# just let me know and I'll suggest some.\n",
    "\n",
    "# Just to reiterate, it doesn't matter if the results are significant or how complicated your\n",
    "# analyses are, what is important is that you are getting practice analyzing data with Python.\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py3]",
   "language": "python",
   "name": "conda-env-py3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
